{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modeling Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "# Set random seed to ensure reproducible results\n",
    "rand_seed = 112\n",
    "seed(rand_seed)\n",
    "set_random_seed(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_planets = pd.read_csv('../clean_planet_data/clean_labeled_planets_seed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_kep = pd.read_csv('../clean_planet_data/clean_labeled_c4_kep_seed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Preprocessing Data:\n",
    "### Mix confirmed planets into data so the model can learn what they are like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7648\n",
       "1      65\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c4_kep['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cut out the extra data to compare across the same timeline (~3200 points)\n",
    "join_planets = has_planets.iloc[:,:3199]\n",
    "\n",
    "# split c4_kep to add to training data\n",
    "to_train_on = c4_kep.head(2705).iloc[:,:3199]\n",
    "# to_train_on.columns = join_planets.columns\n",
    "\n",
    "# set aside the last 5000 stars as a holdout set\n",
    "c4_holdout = c4_kep.tail(5000).iloc[:,:3199]\n",
    "# c4_holdout.columns = join_planets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.concat([join_planets, to_train_on], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Acc:  0.3056203056203056\n",
      "0    2681\n",
      "1    1180\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate Baseline Accuracy\n",
    "val_count = master_df['label'].value_counts()\n",
    "base_acc = val_count[1] / val_count.sum()\n",
    "\n",
    "print('Baseline Acc: ', base_acc)\n",
    "print(val_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Normalize the light curves so that stars of different brightnesses can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make X and y\n",
    "X = master_df.iloc[:, 2:]\n",
    "y = master_df['label']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate StandardScaler as ss\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose to scale each lightcurve row (rather than columns)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "# Scaling\n",
    "scaled_df = ss.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(scaled_df, columns=X_train.columns)\n",
    "\n",
    "test_scaled_df = ss.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(test_scaled_df, columns=X_test.columns)\n",
    "\n",
    "# Transpose back to normal\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into an array and then change the dimensions\n",
    "X_array = np.array(X_train)\n",
    "X_array = np.expand_dims(X_array, axis = 2)\n",
    "\n",
    "# do this for the test set too\n",
    "X_test_array = np.array(X_test)\n",
    "X_test_array = np.expand_dims(X_test_array, axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Unseen Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice out the light curve\n",
    "unseen_data = c4_holdout.iloc[:,2:3199]\n",
    "\n",
    "# Scale\n",
    "unseen_data = unseen_data.T # transpose to scale each lightcurve row (rather than columns)\n",
    "\n",
    "scaled_unseen = ss.fit_transform(unseen_data)\n",
    "unseen_data = pd.DataFrame(scaled_unseen, columns=unseen_data.columns)\n",
    "\n",
    "unseen_data = unseen_data.T # Transpose back to normal\n",
    "\n",
    "# Change the dimensions so it can be put through the neural network\n",
    "array_unseen = np.array(unseen_data)\n",
    "array_unseen = np.expand_dims(array_unseen, axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Define Functions for Evaluating the Results Later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be used later to show how the model learned over the epochs\n",
    "\n",
    "def learning_plots(title, metric, val_metric, y_label, c_train='#1f77b4', c_test='orange'):\n",
    "\n",
    "    # Instantiate plot\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot metric of interest\n",
    "    plt.plot(result.history[metric], color = c_train)\n",
    "    plt.plot(result.history[val_metric], color = c_test)\n",
    "\n",
    "    # Set title\n",
    "    plt.title(title)\n",
    "\n",
    "    # Set axis labels\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xlabel('Epoch (# of iterations)')\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "\n",
    "    # Plot girdlines:\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be used for showing the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(predictions, y_test): #, all_results):\n",
    "\n",
    "\n",
    "    # Calculate \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    total = (tp + tn + fp + fn)\n",
    "\n",
    "    \n",
    "    index_labels = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision']\n",
    "    scores = pd.DataFrame(index = index_labels, columns=['Results'])\n",
    "    \n",
    "# Calculate results\n",
    "    decimals = 3\n",
    "\n",
    "    scores.loc['Accuracy'] = round((tp+tn) / (total), decimals)\n",
    "    scores.loc['Sensitivity'] = round(tp / (tp+fn), decimals)\n",
    "    scores.loc['Specificity'] = round(tn / (tn+fp), decimals)\n",
    "    scores.loc['Precision'] = round(tp / (tp+fp), decimals)\n",
    "\n",
    "    # Display the rounded results\n",
    "    display(scores)\n",
    "    \n",
    "# Calculate values for the confusion matrix\n",
    "\n",
    "    confusion = pd.DataFrame(index= ['Pred. Positive','Pred. Negative', 'Total'])\n",
    "\n",
    "    confusion['Act. Positive'] = tp, fn, (tp + fn)\n",
    "    confusion['Act. Negative'] = fp, tn, (fp + tn)\n",
    "\n",
    "    confusion['Total'] = (tp + fp), (fn + tn), total\n",
    "\n",
    "    display(confusion)\n",
    "    \n",
    "    print((tp + fp), 'predicted to have planets', '\\n',\n",
    "    tp, 'true positive planet stars predicted', '\\n',\n",
    "    round(tp/(tp+fn)*100, 3), '% of all true planets', '\\n') \n",
    "    \n",
    "    # Rate of planets in predictions\n",
    "    tp_rate = (tp / (tp + fp))\n",
    "    # Rate of planets in all unseen test set\n",
    "    all_rate = (tp + fn) / total\n",
    "    \n",
    "    print(round(tp_rate / all_rate, 3), 'times better than chance')\n",
    "    \n",
    "    # False positive rate\n",
    "    print(round(fp/(tp+fp)*100, 3), '% false positive rate')\n",
    "    \n",
    "    return scores, confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be used to plot the auc-roc curve when evaluating the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(y_test, y_hat_proba):\n",
    "# Plot ROC-AUC curve\n",
    "\n",
    "    # Generate False positive rate and True positive rate\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_hat_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot settings\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    line_width = 4\n",
    "    \n",
    "    # Title and labels\n",
    "    plt.title('ROC Curve', fontsize=25, position = (0.2,1))\n",
    "    plt.ylabel('TPR')\n",
    "    plt.xlabel('FPR')\n",
    "    \n",
    "    # Gridlines\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw = line_width)\n",
    "    plt.plot([0, 1], [0,1], lw = line_width, linestyle = '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Neural Network Modeling:\n",
    "### Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Tune further:\n",
    "#     Pooling layers between all convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional and layer.\n",
    "model.add(Conv1D(filters = 15, # tuned for 15\n",
    "                 kernel_size = (22),  # filter size, tuned for 20\n",
    "                 activation = 'relu',\n",
    "                 input_shape = (3197, 1))) # dimensions of training data\n",
    "\n",
    "# model.add(MaxPooling1D(pool_size = (1)))\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(filters = 20, \n",
    "                 kernel_size = 40, # best so far 30\n",
    "                 activation = 'relu'))\n",
    "\n",
    "# model.add(MaxPooling1D(pool_size = (1)))\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(filters = 10, # best so far 10\n",
    "                 kernel_size = 60, # best so far 60\n",
    "                 activation = 'relu'))\n",
    "\n",
    "# Pooling:\n",
    "model.add(MaxPooling1D(pool_size = (3))) # best so far 3\n",
    "model.add(Dropout(0.2)) # best so far .2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(filters = 7, # tuned for 7, new-\n",
    "                 kernel_size = 20, # tuned for 20, new-\n",
    "                 activation = 'relu'))\n",
    "\n",
    "# model.add(Conv1D(filters = 7,\n",
    "#                  kernel_size = 10, # 5?\n",
    "#                  activation = 'relu'))\n",
    "\n",
    "# Pooling layer.\n",
    "model.add(MaxPooling1D(pool_size = (5))) # best so far 5\n",
    "model.add(Dropout(0.4)) # regularization tuned for .25, confirmed 2nd round\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(filters = 10, # tuned to 10, confirmed 2nd round\n",
    "                 kernel_size = 30, # tuned to 10, second round ~30\n",
    "                 activation = 'relu'))\n",
    "\n",
    "# model.add(Conv1D(filters = 7,\n",
    "#                  kernel_size = 5, # 5?\n",
    "#                  activation = 'relu'))\n",
    "\n",
    "# Pooling layer.\n",
    "model.add(MaxPooling1D(pool_size = (5))) # tuned to 5, confirmed 2nd round\n",
    "model.add(Dropout(0.5)) # regularization tuned to .5\n",
    "\n",
    "\n",
    "\n",
    "# Organize neurons by flattening.\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected hidden layers.\n",
    "\n",
    "# Hidden layer 1\n",
    "model.add(Dense(2500, activation = 'relu')) # 1st round- 2500, .7 drop\n",
    "model.add(Dropout(0.4)) # best so far .4\n",
    "\n",
    "model.add(Dense(1500, activation = 'relu')), # 1st round- 1000, .7 drop, 2nd best so far 1500, .4 drop\n",
    "model.add(Dropout(0.5)) # best so far .5?\n",
    "\n",
    "# model.add(Dense(500, activation = 'relu'))\n",
    "\n",
    "\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Setting learning rate and decay\n",
    "learn_rate = 0.001\n",
    "# # x% reduction in learing_rate by epoch: coef_by_epoch\n",
    "# coef_reduce = .75\n",
    "# coef_by_epoch = 30\n",
    "\n",
    "# Calculate decay\n",
    "decay = 0 #(learn_rate - (coef_reduce * learn_rate)) / coef_by_epoch\n",
    "\n",
    "# Changing adam optimization parameters\n",
    "optimizers.adam(lr = learn_rate, decay = decay)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2895 samples, validate on 966 samples\n",
      "Epoch 1/40\n",
      "2895/2895 [==============================] - 121s 42ms/step - loss: 0.6415 - acc: 0.6857 - val_loss: 0.6109 - val_acc: 0.6946\n",
      "Epoch 2/40\n",
      "2895/2895 [==============================] - 118s 41ms/step - loss: 0.6000 - acc: 0.6943 - val_loss: 0.6207 - val_acc: 0.6946\n",
      "Epoch 3/40\n",
      "2895/2895 [==============================] - 119s 41ms/step - loss: 0.5905 - acc: 0.6943 - val_loss: 0.6318 - val_acc: 0.6946\n",
      "Epoch 4/40\n",
      "2895/2895 [==============================] - 118s 41ms/step - loss: 0.5874 - acc: 0.6943 - val_loss: 0.6418 - val_acc: 0.6946\n",
      "Epoch 5/40\n",
      "2895/2895 [==============================] - 120s 41ms/step - loss: 0.5832 - acc: 0.6943 - val_loss: 0.6339 - val_acc: 0.6946\n",
      "Epoch 6/40\n",
      "2895/2895 [==============================] - 134s 46ms/step - loss: 0.5815 - acc: 0.6981 - val_loss: 0.6434 - val_acc: 0.6832\n",
      "Epoch 7/40\n",
      "2895/2895 [==============================] - 125s 43ms/step - loss: 0.5716 - acc: 0.7136 - val_loss: 0.5909 - val_acc: 0.7174\n",
      "Epoch 8/40\n",
      "2895/2895 [==============================] - 120s 42ms/step - loss: 0.5608 - acc: 0.7285 - val_loss: 0.5805 - val_acc: 0.7174\n",
      "Epoch 9/40\n",
      "2895/2895 [==============================] - 127s 44ms/step - loss: 0.5552 - acc: 0.7320 - val_loss: 0.5832 - val_acc: 0.7298\n",
      "Epoch 10/40\n",
      "2895/2895 [==============================] - 125s 43ms/step - loss: 0.5423 - acc: 0.7509 - val_loss: 0.5903 - val_acc: 0.7153\n",
      "Epoch 11/40\n",
      "2895/2895 [==============================] - 130s 45ms/step - loss: 0.5257 - acc: 0.7527 - val_loss: 0.5567 - val_acc: 0.7464\n",
      "Epoch 12/40\n",
      "2895/2895 [==============================] - 129s 45ms/step - loss: 0.5173 - acc: 0.7589 - val_loss: 0.5627 - val_acc: 0.7184\n",
      "Epoch 13/40\n",
      "2895/2895 [==============================] - 130s 45ms/step - loss: 0.5029 - acc: 0.7762 - val_loss: 0.5323 - val_acc: 0.7464\n",
      "Epoch 14/40\n",
      "2895/2895 [==============================] - 132s 46ms/step - loss: 0.4957 - acc: 0.7720 - val_loss: 0.5656 - val_acc: 0.7598\n",
      "Epoch 15/40\n",
      "2895/2895 [==============================] - 132s 45ms/step - loss: 0.4922 - acc: 0.7765 - val_loss: 0.4943 - val_acc: 0.7909\n",
      "Epoch 16/40\n",
      "2895/2895 [==============================] - 130s 45ms/step - loss: 0.4701 - acc: 0.7896 - val_loss: 0.4634 - val_acc: 0.8064\n",
      "Epoch 17/40\n",
      "2895/2895 [==============================] - 137s 47ms/step - loss: 0.4643 - acc: 0.7927 - val_loss: 0.4703 - val_acc: 0.8054\n",
      "Epoch 18/40\n",
      "2895/2895 [==============================] - 126s 44ms/step - loss: 0.4648 - acc: 0.8100 - val_loss: 0.4492 - val_acc: 0.8178\n",
      "Epoch 19/40\n",
      "2895/2895 [==============================] - 126s 43ms/step - loss: 0.4451 - acc: 0.8066 - val_loss: 0.4378 - val_acc: 0.8178\n",
      "Epoch 20/40\n",
      "2895/2895 [==============================] - 131s 45ms/step - loss: 0.4112 - acc: 0.8290 - val_loss: 0.4489 - val_acc: 0.8219\n",
      "Epoch 21/40\n",
      "2895/2895 [==============================] - 131s 45ms/step - loss: 0.4076 - acc: 0.8297 - val_loss: 0.4607 - val_acc: 0.8458\n",
      "Epoch 22/40\n",
      "2895/2895 [==============================] - 135s 46ms/step - loss: 0.4088 - acc: 0.8304 - val_loss: 0.4130 - val_acc: 0.8395\n",
      "Epoch 23/40\n",
      "2895/2895 [==============================] - 137s 47ms/step - loss: 0.3863 - acc: 0.8477 - val_loss: 0.3979 - val_acc: 0.8364\n",
      "Epoch 24/40\n",
      "2895/2895 [==============================] - 127s 44ms/step - loss: 0.3663 - acc: 0.8470 - val_loss: 0.3884 - val_acc: 0.8509\n",
      "Epoch 25/40\n",
      "2895/2895 [==============================] - 132s 45ms/step - loss: 0.3658 - acc: 0.8525 - val_loss: 0.3839 - val_acc: 0.8582\n",
      "Epoch 26/40\n",
      "2895/2895 [==============================] - 126s 43ms/step - loss: 0.3626 - acc: 0.8542 - val_loss: 0.4084 - val_acc: 0.8582\n",
      "Epoch 27/40\n",
      "2895/2895 [==============================] - 128s 44ms/step - loss: 0.3529 - acc: 0.8556 - val_loss: 0.4024 - val_acc: 0.8551\n",
      "Epoch 28/40\n",
      "2895/2895 [==============================] - 129s 44ms/step - loss: 0.3435 - acc: 0.8622 - val_loss: 0.3717 - val_acc: 0.8551\n",
      "Epoch 29/40\n",
      "2895/2895 [==============================] - 126s 43ms/step - loss: 0.3378 - acc: 0.8535 - val_loss: 0.4096 - val_acc: 0.8654\n",
      "Epoch 30/40\n",
      "2895/2895 [==============================] - 132s 46ms/step - loss: 0.3240 - acc: 0.8715 - val_loss: 0.4076 - val_acc: 0.8551\n",
      "Epoch 31/40\n",
      "2895/2895 [==============================] - 131s 45ms/step - loss: 0.3259 - acc: 0.8670 - val_loss: 0.4008 - val_acc: 0.8540\n",
      "Epoch 32/40\n",
      "2895/2895 [==============================] - 137s 47ms/step - loss: 0.3177 - acc: 0.8753 - val_loss: 0.4322 - val_acc: 0.8437\n",
      "Epoch 33/40\n",
      "2895/2895 [==============================] - 131s 45ms/step - loss: 0.3095 - acc: 0.8801 - val_loss: 0.4222 - val_acc: 0.8520\n",
      "Epoch 34/40\n",
      "2895/2895 [==============================] - 134s 46ms/step - loss: 0.3035 - acc: 0.8756 - val_loss: 0.3814 - val_acc: 0.8571\n",
      "Epoch 35/40\n",
      "2895/2895 [==============================] - 139s 48ms/step - loss: 0.3003 - acc: 0.8756 - val_loss: 0.4217 - val_acc: 0.8540\n",
      "Epoch 36/40\n",
      "2895/2895 [==============================] - 142s 49ms/step - loss: 0.2895 - acc: 0.8843 - val_loss: 0.3895 - val_acc: 0.8675\n",
      "Epoch 37/40\n",
      "2895/2895 [==============================] - 127s 44ms/step - loss: 0.2856 - acc: 0.8822 - val_loss: 0.4344 - val_acc: 0.8530\n",
      "Epoch 38/40\n",
      " 512/2895 [====>.........................] - ETA: 1:47 - loss: 0.3073 - acc: 0.8770"
     ]
    }
   ],
   "source": [
    "# Keep track of the runtime:\n",
    "start_time = time.time()\n",
    "\n",
    "# Set the random state\n",
    "rand_seed = 112\n",
    "seed(rand_seed)\n",
    "set_random_seed(rand_seed)\n",
    "\n",
    "# Fit the model:\n",
    "result = model.fit(X_array,\n",
    "                    y_train,\n",
    "                    batch_size = 64,\n",
    "                    epochs = 40,\n",
    "                    verbose = 1,\n",
    "                   validation_data = (X_test_array, y_test))\n",
    "\n",
    "# Print the runtime:\n",
    "print('Runtime: ', round((time.time() - start_time)/60, 3), ' minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Training Plots:\n",
    "Training accuracy and loss functions vs. epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot accuracy over epochs\n",
    "learning_plots(title = 'Model Accuracy', \n",
    "               metric = 'acc', \n",
    "               val_metric = 'val_acc', \n",
    "               y_label = 'Accuracy')\n",
    "\n",
    "# Plot loss score over epoch\n",
    "learning_plots(title = 'Model Loss Score', \n",
    "               metric = 'loss', \n",
    "               val_metric = 'val_loss', \n",
    "               y_label = 'Loss Score',\n",
    "              c_train = 'green',\n",
    "              c_test = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Unseen Data:\n",
    "Now that the model has been fit, lets see if we can use it to detect some planets in data not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "y_unseen = model.predict_classes(array_unseen)\n",
    "# Predict probabilities\n",
    "pred_proba = model.predict_proba(array_unseen)\n",
    "\n",
    "# make df of true labels and index\n",
    "true_y_unseen = c4_holdout['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analyze_results function\n",
    "results_df, confusion_df = analyze_results(predictions = y_unseen, \n",
    "                                           y_test = true_y_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot auc-roc curve\n",
    "plot_roc(true_y_unseen, pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
